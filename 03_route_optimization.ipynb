{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3ff40b9-aeb0-414b-9cad-51ae32d6cdab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Make sure to use the cluster created in notebook `02_compute_setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afbbca13-e716-4d60-a616-5f1a6a6ecedc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Libraries"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install osmnx==2.0.4 protobuf==6.31.1 ortools==9.14.6206 folium==0.20.0 ray[default]==2.41.0 \n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3d2f593-ad3b-48e4-84a8-c3e73055a9e1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Configs"
    }
   },
   "outputs": [],
   "source": [
    "NUM_SHIPMENTS = 40_000\n",
    "num_routes = 320  # total trucks available\n",
    "MAX_EV = 4000 # max capacity\n",
    "MAX_VAN = 8000\n",
    "DEPOT_LAT, DEPOT_LON = 39.7685, -86.1580 # start and end point for each route\n",
    "SOLVER_THINKING_TIME = 2 # how long should the solver spend on each route\n",
    "\n",
    "\n",
    "catalog = \"josh_melton\"\n",
    "schema = \"routing\"\n",
    "shipments_table = f\"{catalog}.{schema}.raw_shipments\"\n",
    "clustered_table = f\"{catalog}.{schema}.shipments_by_route\"\n",
    "optimal_routes_table = f\"{catalog}.{schema}.optimized_routes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f0088b-8036-48f1-8726-d423129dfd59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import ray\n",
    "import os\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster, MAX_NUM_WORKER_NODES\n",
    "from mlflow.utils.databricks_utils import get_databricks_env_vars\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from ortools.constraint_solver import pywrapcp, routing_enums_pb2, routing_parameters_pb2\n",
    "from ortools.util import optional_boolean_pb2 as ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c60c2d24-b404-4658-a537-27a48dddcb55",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ray Info"
    }
   },
   "outputs": [],
   "source": [
    "ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "host_name = ctx.tags().get(\"browserHostName\").get()\n",
    "host_token = ctx.apiToken().get()\n",
    "cluster_id = ctx.tags().get(\"clusterId\").get()\n",
    "\n",
    "response = requests.get(\n",
    "    f'https://{host_name}/api/2.1/clusters/get?cluster_id={cluster_id}',\n",
    "    headers={'Authorization': f'Bearer {host_token}'}\n",
    "  ).json()\n",
    "\n",
    "if \"autoscale\" in response:\n",
    "  min_node = response['autoscale'][\"min_workers\"]\n",
    "  max_node = response['autoscale'][\"max_workers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a5e1c24-306c-471f-9bb2-0c8a9106cf22",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Start Ray"
    }
   },
   "outputs": [],
   "source": [
    "# Cluster cleanup\n",
    "restart = True\n",
    "if restart is True:\n",
    "  try:\n",
    "    shutdown_ray_cluster()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  try:\n",
    "    ray.shutdown()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# Set configs based on your cluster size\n",
    "num_cpu_cores_per_worker = 15 # total cpu to use in each worker node (total_cores - 1 to leave one core for spark)\n",
    "num_cpus_head_node = 8 # Cores to use in driver node (up to total_cores - 1)\n",
    "\n",
    "# Set databricks credentials as env vars\n",
    "mlflow_dbrx_creds = get_databricks_env_vars(\"databricks\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = mlflow_dbrx_creds['DATABRICKS_HOST']\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = mlflow_dbrx_creds['DATABRICKS_TOKEN']\n",
    "\n",
    "ray_conf = setup_ray_cluster(\n",
    "  # min_worker_nodes=min_node,\n",
    "  # max_worker_nodes=max_node,\n",
    "  num_worker_nodes=4,\n",
    "  num_cpus_head_node= num_cpus_head_node,\n",
    "  num_cpus_per_node=num_cpu_cores_per_worker,\n",
    "  num_gpus_head_node=0,\n",
    "  num_gpus_worker_node=0\n",
    ")\n",
    "os.environ['RAY_ADDRESS'] = ray_conf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1701572-7b6b-46cd-bd62-9122028d2efb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Shipment Data"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    shipments_df = spark.read.table(shipments_table) \n",
    "except Exception:\n",
    "    # to see how this was generated, or generate different data yourself, check utils/data_generation.py\n",
    "    shipments_df = spark.read.csv(\"utils/shipments.csv\") \n",
    "    shipments_df.write.mode(\"overwrite\").saveAsTable(shipments_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92039c39-6da5-47f1-86ca-6cf4cb736058",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cluster Shipments"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Helper: two-way median split for overweight routes\n",
    "def median_bisect(sdf, cid, max_weight):\n",
    "    # choose the axis with larger spread\n",
    "    bounds = (sdf.agg(F.max(\"latitude\").alias(\"lat_max\"),\n",
    "                      F.min(\"latitude\").alias(\"lat_min\"),\n",
    "                      F.max(\"longitude\").alias(\"lon_max\"),\n",
    "                      F.min(\"longitude\").alias(\"lon_min\"))\n",
    "                  .collect()[0])\n",
    "    lat_range = bounds.lat_max - bounds.lat_min\n",
    "    lon_range = bounds.lon_max - bounds.lon_min\n",
    "    axis      = \"latitude\" if lat_range >= lon_range else \"longitude\"\n",
    "\n",
    "    # median along that axis (1-% error is fine)\n",
    "    median = sdf.approxQuantile(axis, [0.5], 0.01)[0]\n",
    "\n",
    "    # assign sub-cluster ids\n",
    "    split_df = sdf.withColumn(\"cluster_id\",\n",
    "        F.when(F.col(axis) <= median, F.lit(f\"{cid}-1\"))\n",
    "         .otherwise(                F.lit(f\"{cid}-2\"))\n",
    "    )\n",
    "    return split_df\n",
    "\n",
    "# 2. Main builder\n",
    "def build_clusters(shipments_df, num_routes, max_weight=MAX_VAN):\n",
    "    # 2.1 initial spatial k-means\n",
    "    vec = VectorAssembler(inputCols=[\"latitude\", \"longitude\"],\n",
    "                          outputCol=\"features\").transform(shipments_df)\n",
    "\n",
    "    base_model = KMeans(k=num_routes, seed=1).fit(vec.select(\"features\"))\n",
    "    clustered  = (\n",
    "        base_model.transform(vec)\n",
    "                  .withColumnRenamed(\"prediction\", \"cluster_id\")\n",
    "                  .drop(\"features\")                    # no vector in final table\n",
    "    )\n",
    "\n",
    "    # 2.2 overweight cluster ids\n",
    "    heavy_ids = (\n",
    "        clustered.groupBy(\"cluster_id\")\n",
    "                 .agg(F.sum(\"weight\").alias(\"total_wt\"))\n",
    "                 .filter(F.col(\"total_wt\") > max_weight)\n",
    "                 .select(\"cluster_id\")\n",
    "                 .distinct()\n",
    "                 .collect()\n",
    "    )\n",
    "    heavy_ids = [row[\"cluster_id\"] for row in heavy_ids]\n",
    "    print(\"Over-capacity clusters â†’\", heavy_ids or \"none ðŸŽ‰\")\n",
    "\n",
    "    from functools import reduce\n",
    "\n",
    "    # 2.3 bisect each heavy cluster\n",
    "    if heavy_ids:\n",
    "        heavy_df = clustered.filter(F.col(\"cluster_id\").isin(heavy_ids))\n",
    "        keep_df  = clustered.filter(~F.col(\"cluster_id\").isin(heavy_ids))\n",
    "\n",
    "        # create one DataFrame per overweight cluster\n",
    "        splits = [\n",
    "            median_bisect(\n",
    "                heavy_df.filter(F.col(\"cluster_id\") == cid), cid, max_weight\n",
    "            )\n",
    "            for cid in heavy_ids\n",
    "        ]\n",
    "\n",
    "        # fold the list into a single DF: split_df = splits[0] âˆª splits[1] âˆª â€¦\n",
    "        split_df = reduce(lambda d1, d2: d1.unionByName(d2), splits)\n",
    "\n",
    "        # final merged result\n",
    "        clustered = keep_df.unionByName(split_df)\n",
    "\n",
    "    # 2.4 cast id â†’ string, save & return\n",
    "    return clustered.withColumn(\"cluster_id\", F.col(\"cluster_id\").cast(\"string\"))\n",
    "\n",
    "# 3. Run, save, and sanity check\n",
    "try :\n",
    "    clustered_df = spark.read.table(clustered_table)\n",
    "except Exception:\n",
    "    clustered_df = build_clusters(\n",
    "        shipments_df,\n",
    "        num_routes=num_routes, \n",
    "        max_weight=MAX_VAN,\n",
    "    )\n",
    "    clustered_df.write.mode(\"overwrite\").saveAsTable(clustered_table)\n",
    "\n",
    "    (\n",
    "        clustered_df.groupBy(\"cluster_id\")\n",
    "        .agg(F.count(\"*\").alias(\"num_deliveries\"),\n",
    "            F.sum(\"weight\").alias(\"total_weight\"))\n",
    "        .orderBy(F.col(\"total_weight\").desc())\n",
    "    ).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "569759d8-bca8-499d-92f2-2c5300fa8640",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Route Optimizer"
    }
   },
   "outputs": [],
   "source": [
    "def solve_cluster(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # -------- 1. Build coordinate list -------------------------------------\n",
    "    coords = [(DEPOT_LON, DEPOT_LAT)] + list(zip(pdf[\"longitude\"], pdf[\"latitude\"]))\n",
    "    n      = len(coords)\n",
    "\n",
    "    # -------- 2. Trivial cases ---------------------------------------------\n",
    "    if n == 1:      # depot only  â†’ no route rows\n",
    "        return pd.DataFrame([], columns=[\n",
    "            \"cluster_id\",\"truck_type\",\"route_index\",\n",
    "            \"package_id\",\"latitude\",\"longitude\"\n",
    "        ])\n",
    "\n",
    "    if n == 2:      # depot + one stop\n",
    "        r = pdf.iloc[0]\n",
    "        return pd.DataFrame([{\n",
    "            \"cluster_id\":  str(r[\"cluster_id\"]),\n",
    "            \"truck_type\":  \"EV\" if r[\"weight\"] <= MAX_EV else \"Van\",\n",
    "            \"route_index\": 0,\n",
    "            \"package_id\":  r[\"package_id\"],\n",
    "            \"latitude\":    r[\"latitude\"],\n",
    "            \"longitude\":   r[\"longitude\"],\n",
    "        }])\n",
    "\n",
    "    # -------- 3. Single OSRM /table call -----------------------------------\n",
    "    coord_str = \";\".join(f\"{lon:.6f},{lat:.6f}\" for lon, lat in coords)\n",
    "\n",
    "    # can make this a ray actor if needed\n",
    "    def osrm_table(radius=None):\n",
    "        extra = \"\"\n",
    "        if radius is not None:                         # optional wider snap\n",
    "            radii = \";\".join([str(radius)] * n)\n",
    "            extra = f\"&radiuses={radii}\"\n",
    "        url = (\n",
    "            f\"http://localhost:5100/table/v1/driving/{coord_str}\"\n",
    "            f\"?annotations=duration{extra}\"\n",
    "        )\n",
    "        return requests.get(url, timeout=20).json()\n",
    "\n",
    "    data = osrm_table()          # first try (server default radius 100 m)\n",
    "    if data.get(\"code\") != \"Ok\":\n",
    "        data = osrm_table(radius=400)  # fallback for off-graph points\n",
    "\n",
    "    if data.get(\"code\") != \"Ok\" or \"durations\" not in data:\n",
    "        raise RuntimeError(f\"OSRM error: {data}\")\n",
    "\n",
    "    full_dur = (np.array(data[\"durations\"], dtype=float)).astype(int)\n",
    "\n",
    "    # -------- 4. OR-Tools model --------------------------------------------\n",
    "    demands  = [0] + pdf[\"weight\"].tolist()\n",
    "    capacity = MAX_VAN if sum(demands) > MAX_EV else MAX_EV\n",
    "\n",
    "    manager  = pywrapcp.RoutingIndexManager(n, 1, 0)\n",
    "    routing  = pywrapcp.RoutingModel(manager)\n",
    "\n",
    "    def cost_cb(i, j):\n",
    "        return full_dur[manager.IndexToNode(i)][manager.IndexToNode(j)]\n",
    "      \n",
    "    routing.SetArcCostEvaluatorOfAllVehicles(routing.RegisterTransitCallback(cost_cb))\n",
    "\n",
    "    demand_cb = routing.RegisterUnaryTransitCallback(\n",
    "        lambda idx: demands[manager.IndexToNode(idx)]\n",
    "    )\n",
    "    print(\"demands :\", demands)              # list delivered at each stop\n",
    "    print(\"total   :\", sum(demands))         # vehicle must deliver at least this\n",
    "    print(\"capacity:\", capacity)             # max vehicle load you allow\n",
    "    print(\"max indiv demand:\", max(demands))\n",
    "\n",
    "    # -------- 5. Search parameters ----------------------------------------\n",
    "    sp = pywrapcp.DefaultRoutingSearchParameters()\n",
    "    sp.first_solution_strategy    = routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC\n",
    "    sp.local_search_metaheuristic = routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH # try other heuristics\n",
    "    # sp.time_limit.seconds = min(30, n)\n",
    "    sp.solution_limit = n*10*SOLVER_THINKING_TIME\n",
    "    sp.guided_local_search_lambda_coefficient = 0.2\n",
    "    sp.log_search = True\n",
    "    # ops = sp.local_search_operators\n",
    "    # ops.use_full_path_lns  = ob.BOOL_TRUE\n",
    "    # ops.use_two_opt        = ob.BOOL_TRUE\n",
    "    # ops.use_or_opt         = ob.BOOL_TRUE\n",
    "    # ops.use_lin_kernighan  = ob.BOOL_TRUE   \n",
    "\n",
    "    # -------- 6. Solve & extract route -------------------------------------\n",
    "    sol = routing.SolveWithParameters(sp)\n",
    "    if sol is None:\n",
    "        raise RuntimeError(\"No solution found for cluster\")\n",
    "\n",
    "    idx, visit = routing.Start(0), []\n",
    "    while not routing.IsEnd(idx):\n",
    "        node = manager.IndexToNode(idx)\n",
    "        if node != 0:\n",
    "            visit.append(node - 1)\n",
    "        idx = sol.Value(routing.NextVar(idx))\n",
    "\n",
    "    visited_pdf = pdf.iloc[visit]\n",
    "    truck_type  = \"EV\" if capacity > MAX_EV else \"Van\"\n",
    "\n",
    "    rows = []\n",
    "    for i, (_, r) in enumerate(visited_pdf.iterrows()):\n",
    "        rows.append({\n",
    "            \"cluster_id\":  str(r[\"cluster_id\"]),\n",
    "            \"truck_type\":  truck_type,\n",
    "            \"route_index\": i,        \n",
    "            \"package_id\":  r[\"package_id\"],\n",
    "            \"latitude\":    r[\"latitude\"],\n",
    "            \"longitude\":   r[\"longitude\"],\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc65f3c9-dd4e-4d90-9b5d-aaf9e93f3c6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Test on a small example. Remember that the depot in downtown Indianapolis (not visualized) is technically the start and end of the route, which is why the routes tend to start and end at seemingly odd locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286193dd-8ca3-4fb1-9bbb-c9b83977072c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Small Example"
    }
   },
   "outputs": [],
   "source": [
    "from utils.plotter import plot_route_folium\n",
    "\n",
    "small_example = [\n",
    "    # cluster_id, package_id, latitude,  longitude, weight\n",
    "    (1, \"N1\", 39.8184, -86.1581, 12.0),\n",
    "    (1, \"N2\", 39.8584, -86.2081,  8.5),\n",
    "    (1, \"S1\", 39.7184, -86.1581, 15.0),\n",
    "    (1, \"S2\", 39.6784, -86.1081,  6.0),\n",
    "    (1, \"S3\", 39.6584, -86.2081, 10.0),\n",
    "]\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"cluster_id\",  StringType(), False),\n",
    "    StructField(\"package_id\",  StringType(),  False),\n",
    "    StructField(\"latitude\",    DoubleType(),  False),\n",
    "    StructField(\"longitude\",   DoubleType(),  False),\n",
    "    StructField(\"weight\",  DoubleType(),  False),\n",
    "])\n",
    "\n",
    "small_example_pdf = spark.createDataFrame(small_example, df_schema).toPandas()\n",
    "result_pdf = solve_cluster(small_example_pdf)\n",
    "plot_route_folium(result_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b909f5b-b548-4c64-ac17-b7497e312949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Test on a single cluster. Remember that the depot in downtown Indianapolis (not visualized) is technically the start and end of the route, which is why the routes tend to start and end at seemingly odd locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060ac41a-6e6d-4f23-8ed6-9e8a81f1c798",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "One Route Example"
    }
   },
   "outputs": [],
   "source": [
    "single_cluster_df = clustered_df.where(clustered_df.cluster_id == 1).toPandas()\n",
    "result_df = solve_cluster(single_cluster_df)\n",
    "plot_route_folium(result_df) # Remember the depot (not visualized) is technically the start and end of the route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2db6261-e489-4368-a9d9-5b87d3d3edb0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ray Grouping"
    }
   },
   "outputs": [],
   "source": [
    "spark_df = clustered_df.repartition(\"cluster_id\")   \n",
    "ds = ray.data.from_spark(spark_df)\n",
    "\n",
    "result_ds = (\n",
    "    ds.groupby(\"cluster_id\")         # <â€‘â€‘ each group = one cluster\n",
    "      .map_groups(\n",
    "          solve_cluster,\n",
    "          batch_format=\"pandas\",\n",
    "          num_cpus=1,                # reserve one CPU per group\n",
    "      )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4adf0b02-bb1f-43fa-8844-39bd66dca6df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optimize and Write"
    }
   },
   "outputs": [],
   "source": [
    "# can convert to pandas or write directly to delta \n",
    "# result_ds.to_pandas()\n",
    "\n",
    "volume = \"ray_temp\"\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.{volume}\")\n",
    "\n",
    "tmp_dir_fs = f\"/Volumes/{catalog}/{schema}/{volume}/tempDoc\"\n",
    "dbutils.fs.mkdirs(tmp_dir_fs)\n",
    "os.environ[\"RAY_UC_VOLUMES_FUSE_TEMP_DIR\"] = tmp_dir_fs\n",
    "\n",
    "result_ds.write_databricks_table(optimal_routes_table, mode=\"overwrite\", mergeSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c3227d-57dc-4d77-a7db-5630c444e4e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Results"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(optimal_routes_table).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a659c7a9-2937-48e1-a540-c7741916d20c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The *route* and *table* methods of the OSRM Backend Server are two of several methods available through the server's REST API.  The full list of methods include:\n",
    "</p>\n",
    "\n",
    "* [route](http://project-osrm.org/docs/v5.5.1/api/#route-service) - finds the fastest route between coordinates in the supplied order\n",
    "* [nearest](http://project-osrm.org/docs/v5.5.1/api/#nearest-service) - snaps a coordinate to the street network and returns the nearest n matches\n",
    "* [table](http://project-osrm.org/docs/v5.5.1/api/#table-service) - computes the duration of the fastest route between all pairs of supplied coordinates\n",
    "* [match](http://project-osrm.org/docs/v5.5.1/api/#match-service) - snaps given GPS points to the road network in the most plausible way\n",
    "* [trip](http://project-osrm.org/docs/v5.5.1/api/#trip-service) - solves the Traveling Salesman Problem using a greedy heuristic (farthest-insertion algorithm)\n",
    "* [tile](http://project-osrm.org/docs/v5.5.1/api/#tile-service) - generates Mapbox Vector Tiles that can be viewed with a vector-tile capable slippy-map viewer\n",
    "\n",
    "To make any of these accessible during Spark dataframe processing, simply construct a function around the HTTP REST API call and call via Pandas UDFs or Ray as demonstrated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6c596e-6bea-4f36-b738-f8ab0098c501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the [Databricks License](https://databricks.com/db-license-source).  All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "\n",
    "| library                | description                                                                                      | license      | source                                                    |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------|--------------|-----------------------------------------------------------|\n",
    "| OSRM Backend Server    | High performance routing engine written in C++14 designed to run on OpenStreetMap data           | BSD 2-Clause \"Simplified\" License | https://github.com/Project-OSRM/osrm-backend              |\n",
    "| osmnx                  | Download, model, analyze, and visualize street networks and other geospatial features from OpenStreetMap in Python | MIT License  | https://github.com/gboeing/osmnx                          |\n",
    "| ortools                | Operations research tools developed at Google for combinatorial optimization                     | Apache License 2.0 | https://github.com/google/or-tools                        |\n",
    "| folium                 | Visualize data in Python on interactive Leaflet.js maps                                          | MIT License  | https://github.com/python-visualization/folium            |\n",
    "| dash                   | Python framework for building analytical web applications and dashboards; built on Flask, React, and Plotly.js | MIT License  | https://github.com/plotly/dash                            |\n",
    "| branca                 | Library for generating complex HTML+JS pages in Python; provides non-map-specific features for folium | MIT License  | https://github.com/python-visualization/branca            |\n",
    "| plotly                 | Open-source Python library for creating interactive, publication-quality charts and graphs        | MIT License  | https://github.com/plotly/plotly.py                       |\n",
    "ray |\tFlexible, high-performance distributed execution framework for scaling Python workflows |\tApache2.0 |\thttps://github.com/ray-project/ray"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2804724653930863,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03_route_optimization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
